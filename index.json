[{"uri":"https://Son2110.github.io/fcj-workshop/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Rox accelerates sales productivity with AI agents powered by Amazon Bedrock by Santhan Pamulapati, Andrew Brown, Santhosh Kumar Manavasi Lakshminarayanan, Shriram Sridharan, and Taeuk Kang on 01 OCT 2025 in Amazon Bedrock, Amazon Machine Learning, Customer Solutions\nThis post was co-written with Shriram Sridharan, Taeuk Kang, and Santhosh Kumar Manavasi Lakshminarayanan from Rox.\nRox is building a new revenue operating system for the applied AI era.\nModern revenue teams rely on more data than ever before, such as Customer Relationship Management (CRM) systems, marketing automation, finance systems, support tickets, and live product usage. Though each serves its role, together they create silos that slow sellers down and leave insights untapped.\nRox addresses this by providing a revenue operating system: a unified layer that brings these signals together and equips AI agents to execute go-to-market (GTM) workflows. Instead of reconciling reports or updating fields, sellers get real-time intelligence and automation in their daily flow.\nToday, we’re excited to announce that Rox is generally available, with Rox infrastructure built on AWS and delivered across web, Slack, macOS, and iOS. In this post, we share how Rox accelerates sales productivity with AI agents powered by Amazon Bedrock.\nSolution overview As noted in Rox is transforming revenue teams with AI-driven integration powered by AWS, modern GTM teams need more than a static database. Revenue data spans dozens of systems, such as product usage, finance, and support, and teams require a system that unifies context and acts on it in real time.\nRox delivers this through a layered architecture on AWS:\nSystem of record – A unified, governed knowledge graph consolidates CRM, finance, support, product telemetry, and web data. Agent swarms – Intelligent, account-aware agents reason over the graph and orchestrate multi-step workflows like research, outreach, opportunity management, and proposal generation. Interfaces across surfaces – Sellers engage these workflows where they work, such as web application, Slack, iOS, and macOS. This converts the CRM from a passive system of record into an active system of action, so teams can act on their data immediately and intelligently.\nThe following diagram illustrates the solution architecture.\nBenefits and features of ROX Now generally available, Rox extends from intelligence to full execution with Command, a new conversational interface that orchestrates multi-agent workflows. Command coordinates with multiple specialized agents running in parallel.\nA single request (for example, “prep me for the ACME renewal and draft follow-ups”) expands into a plan:\nResearch usage and support signals Identify missing stakeholders Refresh enrichment Propose next-best actions Draft outreach Update the opportunity Assemble a proposal Each step is completed through tool calls into your systems and is subject to guardrail approvals. Our comprehensive safety architecture employs a sophisticated multi-layer guardrail system as the first line of defense against inappropriate, harmful, or malicious requests. Incoming requests undergo rigorous analysis through our advanced filtering mechanisms before reaching the inference layer. This preprocessing stage evaluates multiple dimensions of safety and appropriateness, such as legal compliance assessment and business relevance evaluation, to make sure only legitimate, safe, and contextually appropriate requests proceed to model execution.\nCommand decomposes the request, routes steps to the right agents, sequences external tool invocations (CRM, calendar, enrichment, email), reconciles results into the system of context, and returns one coherent thread that’s ready for consumption on the web, Slack, iOS, or macOS. Every suggestion is explainable (sources and traces), reversible (audit logs), and policy-aware (role-based access control, rate limits, required approvals).\nHow Amazon Bedrock powers Rox Command demands a model capable of reasoning across multiple steps, orchestrating tools, and adapting dynamically.\nTo meet these needs, Rox chose Anthropic’s Claude Sonnet 4 on Amazon Bedrock. Anthropic’s Claude Sonnet 4 has consistently demonstrated unmatched tool-calling and reasoning performance, allowing Rox agents to sequence workflows like account research, enrichment, outreach, opportunity management, and proposal generation with reliability.\nAmazon Bedrock provides the foundation to deliver Rox at enterprise scale, offering security, flexibility to integrate with the latest models, and scalability to handle thousands of concurrent agents reliably.\nAdditional Features In addition to Command, Rox includes the following features:\nFeature Description Research Offers deep account and market research, grounded in unified context (carried over from private beta) Meet Makes it possible to record, transcribe, summarize, and turn meetings into actions (carried over from private beta) Outreach Provides personalized prospect engagement, contextualized by unified data (new) Revenue Helps you track, update, and advance pipelines in the flow of work (new) Auto-fill proposals Helps you assemble tailored proposals in seconds from account context (new) Rox apps Offers modular extensions that add purpose-built workflows (dashboards, trackers) directly into the system (new) iOS app Delivers notifications and meeting prep on the go (new) Mac app Brings the ability to transcribe calls and add them to the system of context (new) Regional expansion Now live in the AWS Middle East (Bahrain) AWS Region, aligning with data residency and sovereignty needs (new) Early customer impact In beta, enterprises saw immediate gains:\n50% higher representative productivity 20% faster sales velocity Twofold revenue per rep For example, real Rox customers were able to sharpen their focus on high-value opportunities, driving a 40–50% increase in average selling price. Another customer saw 90% reduction in rep prep time and faster closes, plus 15% more six-figure deals uncovered through Rox insights. Rox also shortens ramp time for new reps, with customers reporting 50% quicker ramp time using Rox.\nTry Rox today Our vision is for revenue teams to run with an always-on agent swarm that continuously researches accounts, engages stakeholders, and moves the pipeline forward.\nRox is now generally available. Get started at rox.com or visit the AWS Marketplace. Together with AWS, we will continue to build the AI-based operating system for modern revenue teams.\nAbout the authors Shriram Sridharan is the Co-Founder/Engineering Head of Rox, a Sequoia backed AI company. Before Rox, Shriram led the data infrastructure team at Confluent responsible for making Kafka faster and cheaper across clouds. Prior to that he was one of the early engineers in Amazon Aurora (pre-launch) re-imagining databases for the cloud. Aurora was the fastest growing AWS Service and a recipient of the 2019 SIGMOD systems award.\nTaeuk Kang is a Founding Engineer at Rox, working across AI research and engineering. He studied Computer Science at Stanford. Prior to Rox, he built large language model agents and retrieval-augmented generation systems at X (formerly Twitter) and designed the distributed LLM infrastructure powering core product features and Trust \u0026amp; Safety, improving overall platform health. Earlier at Stripe, he developed high-performance streaming and batch data processing pipelines integrating Apache Flink, Spark, Kafka, and AWS SQS.\nSanthosh Kumar Manavasi Lakshminarayanan leads Platform at Rox. Before Rox he was Director of Engineering at StreamSets, acquired by IBM leading StreamSets Cloud Platform making it seamless for big enterprises to run their data pipeline at scale on modern cloud providers. Before StreamSets, he was an senior engineer at Platform Metadata team at Informatica.\nAndrew Brown is an Account Executive for AI Startups at Amazon Web Services (AWS) in San Francisco, CA. With a strong background in cloud computing and a focus on supporting startups, Andrew specializes in helping companies scale their operations using AWS technologies.\nSanthan Pamulapati is a Sr. Solutions Architect for GenAI startups at AWS, with deep expertise in designing and building scalable solutions that drives customer growth. He has strong background in building HPC systems leveraging AWS services and worked with strategic customers to solve business challenges.\n"},{"uri":"https://Son2110.github.io/fcj-workshop/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"How Laravel Nightwatch handles billions of observability events in real time with Amazon MSK and ClickHouse Cloud by Masudur Rahaman Sayem, James Carpenter, Jess Archer, and Johnny Mirza on 01 OCT 2025 in Amazon Managed Streaming for Apache Kafka (Amazon MSK), Analytics, Expert (400), Technical How-to\nLaravel, one of the world’s most popular web frameworks, launched its first-party observability platform, Laravel Nightwatch, to provide developers with real-time insights into application performance. Built entirely on AWS managed services and ClickHouse Cloud, the service already processes over one billion events per day while maintaining sub-second query latency, giving developers instant visibility into the health of their applications.\nBy combining Amazon Managed Streaming for Apache Kafka (Amazon MSK) with ClickHouse Cloud and AWS Lambda, Laravel Nightwatch delivers high-volume, low-latency monitoring at scale, while maintaining the simplicity and developer experience Laravel is known for.\nThe challenge: Delivering real-time monitoring for a global developer community The Laravel framework powers millions of applications worldwide, serving billions of requests each month. Each request can generate potentially hundreds of observability events, such as database queries, queued jobs, cache lookups, emails, notifications, and exceptions. For Nightwatch’s launch, Laravel anticipated instant adoption from its global community, with tens of thousands of applications sending events around the clock from day one.\nLaravel Nightwatch needed an architecture that could:\nIngest millions of JSON events per second from customer applications reliably. Provide sub-second analytical queries for real-time dashboards. Scale horizontally to handle unpredictable traffic spikes. Deliver all of this in a cost-effective, low-maintenance manner. The challenge was to process data on a global scale and provide deep insights into application health without compromising on a straightforward setup experience for developers.\nThe solution: A decoupled streaming and analytics pipeline Laravel Nightwatch implemented a dual-database, streaming-first architecture, shown in the preceding figure, that separates transactional and analytical workloads.\nTransactional workloads – user accounts, organization settings, billing, and similar workloads run on Amazon RDS for PostgreSQL. Analytical workloads – telemetry events, metrics, query logs, and request traces are handled by ClickHouse Cloud. Key components The key components of the solution include the following:\nIngestion layer Amazon API Gateway receives telemetry from Laravel agents embedded in customer applications. Lambda validates and enriches events. Validated and enriched events are published to Amazon MSK, partitioned for scalability. Streaming to analytics ClickPipes in ClickHouse Cloud subscribe directly to MSK topics, reducing the need to build and manage extract, transform, and load (ETL) pipelines. Materialized views in ClickHouse pre-aggregate and transform raw JSON into query-ready formats. Dashboards and delivery The Nightwatch dashboard, built with Laravel, Inertia, and React, runs on AWS Fargate for Amazon ECS. Amazon ElastiCache for Redis accelerates session and cache lookups. Cloudflare CDN provides low-latency delivery to global users. Why Amazon MSK and ClickHouse Cloud? Nightwatch requires a durable, horizontally scalable, and low maintenance streaming backbone.\nWith Amazon MSK Express brokers, we have achieved over 1 million events per second during load testing, benefiting from low-latency, elastic scaling, and simplified operations. MSK Express brokers require no storage sizing or provisioning, scale up to 20 times faster, and recover 90% quicker than standard Apache Kafka brokers—all while enforcing best-practice defaults and client quotas for reliable performance. Its seamless integration with other AWS services—such as Lambda, Amazon Simple Storage Service (Amazon S3), and Amazon CloudWatch—made it straightforward to build a resilient, end-to-end streaming architecture.\nTo ingest and transform these events in real time, Nightwatch uses ClickHouse Cloud and its managed integration platform, ClickPipes. ClickHouse Cloud excels at analytical workloads by delivering up to 100 times faster query performance for analytics compared to traditional row-based databases. Its advanced compression algorithms provide up to 90% storage savings, significantly reducing infrastructure costs while maintaining high performance. With its columnar architecture and optimized execution engine, ClickHouse Cloud can query billions of rows in under 1 second, enabling Laravel Nightwatch to serve real-time dashboards and analytics at global scale.\nBy integrating Amazon MSK and ClickHouse using ClickPipes, Laravel also reduced the operational burden of building and managing ETL pipelines, reducing latency and complexity.\nOvercoming challenges Testing complexity While synthetic benchmarking and test datasets yield useful results, a more realistic workload is required to rigorously test infrastructure and code before deployment to production. The team used Terraform to manage infrastructure alongside application code, creating multiple dev and test environments, and allowing them to test the platform internally with their own applications before each release.\nMulti-region infrastructure The need to cater to multiple data storage regions also brought challenges—with latency, complexity, and cost the foremost concerns. However, the AWS, ClickHouse Cloud, and Cloudflare stack made available a powerful set of networking tools and scaling options. While VPC peering, RDS replication, and global server load balancing did the heavy lifting on the networking side, the ability to scale and right-size each resource kept costs to a minimum.\nQuery performance at scale Materialized views, intelligent time-series partitioning, and specialized ClickHouse codecs helped ensure that queries remained sub-second even as data volumes grew into the billions. Meanwhile, compute separation allowed distinct workloads to scale separately while accessing the same data, with clusters right-sized horizontally and vertically depending on the requirements of each load.\nResults Laravel Nightwatch’s launch exceeded expectations:\n5,300 users registered in the first 24 hours 500 million events processed on day one 97 ms average dashboard request latency 760,000 exceptions logged and analyzed in real time By building on Amazon MSK and ClickHouse Cloud, we were able to scale from zero to billions of events without sacrificing performance or developer experience.\nWhat’s next Laravel plans to expand Nightwatch with:\nMore regions to cater to customers with data sovereignty requirements outside the US and EU. Broader data collection to provide even deeper insight into customers’ applications. SOC 2 certification to cater to customers with tighter compliance requirements. More advanced monitoring and analysis to identify issues before they affect users. The current architecture comfortably supports applications of all sizes, from hobby to enterprise (including a generous free tier), and is designed to handle over one trillion monthly events without performance degradation.\nConclusion Laravel Nightwatch demonstrates how Amazon MSK, ClickHouse Cloud, and AWS serverless technologies can be combined to build a cost-effective, real-time monitoring platform at global scale. By designing for scale from day one, Laravel delivered sub-second analytics across billions of events, while maintaining the developer-friendly experience their community expects.\nAbout the authors Jess Archer is an Engineering Manager and Head of Nightwatch at Laravel, focusing on application observability, performance monitoring, and developer experience. She leads the Nightwatch team while staying hands-on in the codebase. Prior to Laravel, Jess worked on clinical data collection platforms, software for law enforcement, and anti-phishing solutions in banking. She later contributed extensively to Laravel’s open-source ecosystem before moving into her current leadership role. Jess is deeply passionate about open source and creating tools that make developers more productive.\nJames Carpenter is a Senior Infrastructure Engineer joined Laravel in 2024 as Infrastructure Lead for the Nightwatch team, bringing experience from 15 years in sport and healthcare. Specialising in DevOps and Infrastructure, he is passionate about solving complex problems and creating exceptional experiences for both customers and developers.\nJohnny Mirza is a Solution Architect with ClickHouse, working with users across APAC. With over 20 years of background in solutions engineering, he’s experienced in architecting and enabling solutions for enterprise clients in the telecommunications, media, insurance, and financial services sectors. Johnny has a high level of expertise of integration between both public cloud and on-premise infrastructure, while focussing on service assurance, monitoring platforms, and open-source technologies. Prior to ClickHouse, Johnny was part of the solution engineering teams at Confluent, Splunk, and Optus, to name a few.\nMasudur Rahaman Sayem is a Streaming Data Architect at AWS with over 25 years of experience in the IT industry. He collaborates with AWS customers worldwide to architect and implement sophisticated data streaming solutions that address complex business challenges. As an expert in distributed computing, Sayem specializes in designing large-scale distributed systems architecture for maximum performance and scalability. He has a keen interest and passion for distributed architecture, which he applies to designing enterprise-grade solutions at internet scale.\n"},{"uri":"https://Son2110.github.io/fcj-workshop/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"How to export to Amazon S3 Tables by using AWS Step Functions Distributed Map by Chetan Makvana and Aidan Eglin on 01 OCT 2025 in Advanced (300), Amazon S3 Tables, AWS Step Functions, Serverless, Technical How-to\nCompanies running serverless workloads often need to perform extract, transform, and load (ETL) operations on data files stored in Amazon Simple Storage Service (Amazon S3) buckets. Though traditional approaches such as an AWS Lambda trigger for Amazon S3 or Amazon S3 Event Notifications can handle these operations, they might fall short when workflows require enhanced visibility, control, or human intervention. For example, some processes might need manual review of failed records or explicit approval before proceeding to subsequent stages. Customer orchestration solutions to these issues can prove to be complex and error prone.\nAWS Step Functions address these challenges by providing built-in workflow management and monitoring capabilities. The Step Functions Distributed Map feature is designed for high-throughput, parallel data processing workflows so that companies can handle complex ETL jobs, fan-out processing, and data visualization at scale. Distributed Map handles each dataset item as an independent child workflow, processing millions of records while maintaining built-in concurrency controls, fault tolerance, and progress tracking. The processed data can be seamlessly exported to various destinations, including Amazon S3 Tables with Apache Iceberg support.\nIn this post, we show how to use Step Functions Distributed Map to process Amazon S3 objects and export results to Amazon S3 Tables, creating a scalable and maintainable data processing pipeline.\nSee the associated GitHub repository for detailed instructions about deploying this solution as well as sample code.\nSolution overview Consider a consumer electronics company that regularly participates in industry trade shows and conferences. During these events, interested attendees fill out paper sign-up forms to request product demos, receive newsletters, or join early access programs. After the events, the company’s team scans hundreds of thousands of these forms and uploads them to Amazon S3. Rather than manually reviewing each form, the company wants to automate the extraction of key customer details such as name, email address, mailing address, and interest areas. They’d like to store this structured data in S3 Tables with Apache Iceberg format for downstream analytics and marketing campaign targeting.\nLet’s look at how this post’s solution uses Distributed Map to process PDFs in parallel, extract data using Amazon Textract, and write the cleaned output directly to S3 Tables. The result is scalable, serverless post-event data onboarding, as shown in the following figure.\nThe data processing workflow as shown in the preceding diagram includes the following steps:\nA user uploads customer interest forms as scanned PDFs to an Amazon S3 bucket. An Amazon EventBridge Scheduler rule triggers at regular intervals, initiating a Step Functions workflow execution. The workflow execution activates a Step Functions Distributed Map state, which lists all PDF files uploaded to Amazon S3 since the previous run. The Distributed Map iterates over the list of objects and passes each object’s metadata (bucket, key, size, entity tag [ETag]) to a child workflow execution. For each object, the child workflow calls Amazon Textract with the provided bucket and key to extract raw text and relevant fields (name, email address, mailing address, interest area) from the PDF. The child workflow sends the extracted data to Amazon Data Firehose, which is configured to forward data to S3 Tables. Firehose batches the incoming data from the child workflow and writes it to S3 Tables at a preconfigured time interval of your choosing. With data now structured and accessible in S3 Tables, users can easily analyze them using standard SQL queries with Amazon Athena or business intelligence like Amazon QuickSight.\nThe data-processing workflow EventBridge Scheduler starts new Step Functions workflows at regular intervals. The timeline for this schedule is flexible. However, When setting up your schedule, make sure the frequency aligns with how far back your state machine is configured to look for PDFs. For example, if your state machine checks for PDFs from the past week, you’d want to schedule it to run weekly. The Step Functions workflow subsequently performs the following three steps (note that these steps are steps 4, 5, 6, and 7 in the preceding workflow diagram):\nExtract relevant user data from the PDFs. Send the extracted user data to Firehose. Write the data to S3 Tables in Apache Iceberg table format. The following diagram illustrates this workflow.\nLet’s look at each step of the preceding workflow in more detail.\nExtract relevant user data from PDF documents Step Functions uses Distributed Map to process PDFs concurrently in parallel child workflows. It accepts input from JSON, JSONL, CSV, Parquet files, Amazon S3 manifest files stored in Amazon S3 (used to specify particular files for processing), or an Amazon S3 bucket prefix (allows iteration over file metadata for all objects under that prefix). The Step Functions automatically handles parallelization by splitting the dataset and running child workflows for each item, with the ItemBatcher field allowing to group multiple PDFs into a single child workflow execution (e.g., 10 PDFs per batch) to optimize performance and cost.\nThe following screenshot of the Step Functions console shows the configuration for Distributed Map. For example, we have configured Distributed Map to process 10 customer interest PDFs in a single child workflow.\nThe following image shows one example of these scanned PDFs, which includes the customer information that this post’s solution processes.\nEach child workflow then calls the Amazon Textract AnalyzeDocument API with specific queries to extract customer information.\n{ \u0026#34;Document\u0026#34;: { \u0026#34;S3Object\u0026#34;: { \u0026#34;Bucket\u0026#34;: \u0026#34;\u0026lt;input PDFs bucket\u0026gt;\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;{% $states.input.Key %}\u0026#34; } }, \u0026#34;FeatureTypes\u0026#34;: [ \u0026#34;QUERIES\u0026#34; ], \u0026#34;QueriesConfig\u0026#34;: { \u0026#34;Queries\u0026#34;: [ { \u0026#34;Alias\u0026#34;: \u0026#34;full_name\u0026#34;, \u0026#34;Text\u0026#34;: \u0026#34;What is the customer\u0026#39;s name?\u0026#34; }, { \u0026#34;Alias\u0026#34;: \u0026#34;phone_number\u0026#34;, \u0026#34;Text\u0026#34;: \u0026#34;What is the customer’s phone number?\u0026#34; }, { \u0026#34;Alias\u0026#34;: \u0026#34;mailing_address\u0026#34;, \u0026#34;Text\u0026#34;: \u0026#34;What is the customer’s mailing address?\u0026#34; }, { \u0026#34;Alias\u0026#34;: \u0026#34;interest\u0026#34;, \u0026#34;Text\u0026#34;: \u0026#34;What is the customer’s interest?\u0026#34; } ] } } The API analyzes each scanned PDF and returns a JSON structure containing the extracted customer information.\nSend the extracted user data to Firehose The child workflow then uses a Firehose PutRecordBatch API action with service integrations to queue the extracted customer information for further processing. The PutRecordBatch action request includes the Firehose stream name and the data records. The data records include a data blob from step 1 that contains extracted customer information, as shown in the following example.\n{ \u0026#34;DeliveryStreamName\u0026#34;: \u0026#34;put_raw_form_data_100\u0026#34;, \u0026#34;Records\u0026#34;: [ { \u0026#34;Data\u0026#34;: \u0026#34;{\\\u0026#34;full_name\\\u0026#34;:\\\u0026#34;Anthony Ayala\\\u0026#34;,\\\u0026#34;phone_number\\\u0026#34;:\\\u0026#34;001-384-925-0701\\\u0026#34;,\\\u0026#34;mailing_address\\\u0026#34;:\\\u0026#34;38548 Joshua Wall Suite 974, East Heatherfort, OH 32669\\\u0026#34;,\\\u0026#34;interest\\\u0026#34;:\\\u0026#34;Fitness Trackers\\\u0026#34;,\\\u0026#34;processed_date\\\u0026#34;:\\\u0026#34;2025-05-01\\\u0026#34;}\u0026#34; }, { \u0026#34;Data\u0026#34;: \u0026#34;{\\\u0026#34;full_name\\\u0026#34;:\\\u0026#34;Becky Williams\\\u0026#34;,\\\u0026#34;phone_number\\\u0026#34;:\\\u0026#34;+1-283-499-2466\\\u0026#34;,\\\u0026#34;mailing_address\\\u0026#34;:\\\u0026#34;227 King Forge Suite 241, East Nathanland, PR 05687\\\u0026#34;,\\\u0026#34;interest\\\u0026#34;:\\\u0026#34;Al Assistants\\\u0026#34;,\\\u0026#34;processed_date\\\u0026#34;:\\\u0026#34;2025-05-01\\\u0026#34;}\u0026#34; } ] } Write the data to S3 Tables in Apache Iceberg table format Firehose efficiently manages data buffering, format conversion, and reliable delivery to various destinations, including Apache Iceberg, raw files in Amazon S3, Amazon OpenSearch Service, or any of the other supported destinations. Apache Iceberg tables can be either self-managed in Amazon S3 or hosted in S3 Tables. Though self-managed Iceberg tables require manual optimization—such as compaction and snapshot expiration—S3 Tables automatically optimize storage for large-scale analytics workloads, improving query performance and reducing storage costs.\nFirehose simplifies the process of streaming data by configuring a delivery stream, selecting a data source, and setting an Iceberg table as the destination. After you’ve set it up, the Firehose stream is ready to deliver data. The delivered data can be queried from S3 Tables by using Athena, as shown in the following screenshot of the Athena console.\nThe query results include all processed customer data from the PDFs, as shown in the following screenshot.\nThis integration demonstrates a powerful, code-free solution for transforming raw PDF forms into enriched, queryable data in an Iceberg table. You can use these data for further analysis.\nConclusion In this post, we showed how to build a scalable, serverless solution for processing PDF documents and exporting the extracted data to S3 Tables by using Step Functions Distributed Map. This architecture offers several key benefits such as reliability, cost-effectiveness, visibility, and maintainability. By leveraging AWS services such as Step Functions, Amazon Textract, Firehose, and S3 Tables, companies can automate their document processing workflows while ensuring optimal performance and operational excellence. This solution can be adapted for various use cases beyond customer interest forms, such as invoice processing, application forms, or any scenario requiring structured data extraction from documents at scale.\nThough this example focuses on processing PDF data and writing to S3 Tables, Distributed Map can handle various input sources including JSON, JSONL, CSV, and Parquet files in Amazon S3; items in Amazon DynamoDB tables; Athena query results; and all paginated AWS List APIs. Similarly, through Step Functions service integrations, you can write results to multiple destinations such as DynamoDB tables by using the PutItem service integration.\nTo get started with this solution, see the associated GitHub repository for deployment instructions and sample code.\n"},{"uri":"https://Son2110.github.io/fcj-workshop/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS GenAI \u0026amp; Data\u0026rdquo; Event Objectives Understand how to build a strong Data Foundation for AI. Update on the roadmap and adoption trends of GenAI on AWS. Learn about the new concept: AI-Driven Development Lifecycle (AI-DLC). Deep dive into security for Generative AI applications. Explore the power of AI Agents beyond simple automation. Speakers Jun Kai Loke - AI/ML Specialist SA, AWS Kien Nguyen - Solutions Architect, AWS Tamelly Lim - Storage Specialist SA, AWS Binh Tran - Senior Solutions Architect, AWS Taiki Dang - Solutions Architect, AWS Michael Armentano - Principal WW GTM Specialist, AWS Key Highlights Unified Data Foundation The Foundation: Before doing any cool AI stuff, we need a solid data infrastructure. Components: Learned about ingestion, storage, and governance to handle modern data demands. AI-Driven Development Lifecycle (AI-DLC) Shift in Mindset: It\u0026rsquo;s not just using AI as a helper; it\u0026rsquo;s about embedding AI as a central collaborator in the whole software process. Human + AI: Combining AI execution with human oversight to speed up development and improve quality. Securing GenAI Security Layers: Security isn\u0026rsquo;t just one thing; it covers infrastructure, models, and the apps themselves. Zero-Trust: Learned about encryption and access controls to keep data safe throughout the AI lifecycle. AI Agents Productivity Multipliers: Moving from manual tasks to having intelligent partners (Agents) that learn and execute complex tasks autonomously. Key Takeaways Data First I realized that \u0026ldquo;Data is the fuel.\u0026rdquo; Without a unified and scalable data foundation, advanced AI initiatives are impossible. AI as a Collaborator The concept of AI-DLC was new to me. It changed my view from \u0026ldquo;AI writes code for me\u0026rdquo; to \u0026ldquo;AI works with me\u0026rdquo; to drive innovation. Security is Critical Securing GenAI is complex. I learned that we need to protect the data confidentiality and integrity at every stage, not just at the end. Applying to Work Check Data: I will look at how my current projects store data and see if it\u0026rsquo;s \u0026ldquo;AI-ready.\u0026rdquo; Explore Agents: I want to try building a simple AI Agent to automate some of my daily manual tasks. Security Awareness: I will pay more attention to encryption and access control when experimenting with Bedrock models. Event Experience Attending this event gave me a comprehensive view of the GenAI landscape on AWS.\nLearning from the Experts The session by Binh Tran on AI-DLC was particularly eye-opening. It made me rethink how software engineering will look in the near future. Michael Armentano\u0026rsquo;s talk on AI Agents made me very excited about the potential of boosting productivity exponentially. Understanding the Ecosystem I saw how different pieces fit together: from the raw data layer (Storage/Glue) to the security layer, and finally to the application layer with Agents. Some event photos Overall, this event was a deep dive into the future of tech. It wasn\u0026rsquo;t just about \u0026ldquo;hype\u0026rdquo;; it provided a structured roadmap on how to actually implement and secure GenAI solutions.\n"},{"uri":"https://Son2110.github.io/fcj-workshop/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Ly Hoang Son\nPhone Number: 0853596459\nEmail: lyhoangson211002005@gmail.com\nUniversity: FPT University Ho Chi Minh\nMajor: Software Engineer\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 09/06/2025 to 12/24/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://Son2110.github.io/fcj-workshop/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations - Create AWS Free Tier account 09/08/2025 09/08/2025 https://000001.awsstudygroup.com/ 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; - Practice: + Set up MFA + Create AWS budget 09/09/2025 09/09/2025 https://000007.awsstudygroup.com/ 4 - Learn about Amazon VPC - Learn about Security Group, Direct Connect, Load Balancer, Extra Resources - Practice: + Create VPC, Subnet, Internet Gatewway, Route Table, Security Group + Enalbe VPC flow logs 09/10/2025 09/10/2025 \u0026lt; https://000003.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS - SSH connection methods to EC2 - Learn about Elastic IP - Practice: + Launch an EC2 server + Test connection + Create Nat Gateway 09/11/2025 09/11/2025 https://000003.awsstudygroup.com/vi/4-createec2server// 6 - Review VPC, EC2, AWS services - Learn how to draw AWS architecture - Practice + Draw AWS architecture on Draw.io 09/12/2025 09/12/2025 https://www.youtube.com/watch?v=l8isyDe-GwY\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=2 Week 1 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n"},{"uri":"https://Son2110.github.io/fcj-workshop/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Understand EC2, CloudFormation, VPC peering, AWS Transit Gateway Kwow how to launch Amazon Linux Instance and Microsoft Windows Instance Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Practice: + Create Linux and Window VPC + Create SC for Linux and Windows Instance + Launch Linux and Windows Instance + Connect to Linux and Windows Instance + Deploy an AWS User Management Application on Amazon EC2 Linux/Windows 09/15/2025 09/15/2025 https://000004.awsstudygroup.com/ 3 - Learn about Hybrid DNS with route 53 + Outbound Endpoints + Inbound Endpoints + Route 53 Resolver Rules - Practice: + Create key pair + Create CloudFormation Template, SC Connect to RDGW 09/16/2025 09/16/2025 https://000010.awsstudygroup.com/ 4 - Learn about VPC peering - Practice: + Create VPC peering 09/17/2025 09/17/2025 https://000019.awsstudygroup.com/ 5 - Participate in AWS Cloud Day 09/18/2025 09/18/2025 6 - Learn about AWS Transite Gateway Practice: + Create AWS Transite Gateway 09/19/2025 09/19/2025 https://000020.awsstudygroup.com/ Week 2 Achievements: Understood what EC2 is and mastered the basic Ec2:\nInstance Type AMI Key Pair Backup Successfully launch an Instance from a custom AMI and access to Windows/Linux Instance when losing key pair\nSuccessfully created Hybrid DNS with Route 53 Resolver, connect to RDGW and set up DNS:\nOutbound Endpoint Inbound Endpoint Route 53 Resovler Rules Understood about VPC Peering and set up:\nCloudFormation Template Security Group EC2 Instance Network ACL Route Tables Cross-Peer DNS Understood about AWS Transite Gateway and know to create:\nTransite Gateway Transite Gateway Attachment Transite Gateway Route Tables Add Transite Gateway Routes to VPC Route Tables "},{"uri":"https://Son2110.github.io/fcj-workshop/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Understande about EC2 and S3 services Can import virtual machine to AWS Can export virtual machine from EC2 instance and AMI Can mount a file sharing on on-premise machine Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about EC2 basic + Instance Type + User data + Meta data + EC2 auto scailing - EFS/FSx - Lightsail - MGN 09/22/2025 09/22/2025 https://www.youtube.com/watch?v=e7XeKdOVq40\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=73 3 - Practice + Create S3 Bucket + Load data + Deploy web using S3 + Accelerate Static Web with Cloudfront 09/23/2025 09/23/2025 https://000057.awsstudygroup.com/ 4 - Practice + Create Storage Gatewayy + Create File Shares + Mount File Shares on On-premises machine + Create backup plan + Set up notifications 09/24/2025 09/24/2025 https://000024.awsstudygroup.com/ https://000013.awsstudygroup.com/ 5 - Learn about S3: + Access Point + Storage class + Static Website \u0026amp; CORS + Glacier + Snow Family - Store Gateway - Backup 09/25/2025 09/25/2025 https://www.youtube.com/watch?v=hsCfP0IxoaM\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=103 6 - Practice: + Import virtual machine to AWS + Deploy Instance from AMI + Export virtual machine from EC2 instance and from an AMI through an S3 bucket 09/26/2025 09/26/2025 https://000014.awsstudygroup.com/ Week 3 Achievements: Understood EC2 and mastered the basic service group:\nUser/Meta data EC2 auto scailing EFS/FSx Lightsail MGN Understood what S3 is and mastered :\nAccess point Storage class Static Website \u0026amp; CORS Glacier Successfully created and configured Static Web by using S3.\nSuccessfully import virtual machine to AWS\nSuccessfully export EC2 Instance from AWS/AMi\nSucessfully mount a file sharing on on-premise machine\n"},{"uri":"https://Son2110.github.io/fcj-workshop/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Understand basic about security on AWS Know how to manage resources by using tags Know to create IAM role and policy Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Practice: + Create CloudFormation + Create Amazon FSx (SSD \u0026amp; HDD) + Enable shadow copies, user storage quotas and continous access share + Scale storage and thoughput capacity 09/29/2025 09/29/2025 https://000025.awsstudygroup.com/ 3 - Learn about security services on AWS + Shared Responsibility Model + AWS Identity and Acess Management + Amazon Cognito + AWS Organization + KMS 09/30/2025 09/30/2025 https://www.youtube.com/watch?v=tsobAlSg19g\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=150 4 - Do 3 labs 18, 22, and 27 - Practice: + Enable Security Hub + Create tags for Instances + Create a role for Lambda function + Mange resources by using tags and resources groups 10/01/2025 10/01/2025 https://000022.awsstudygroup.com/ https://000027.awsstudygroup.com/ https://000018.awsstudygroup.com/ 5 - Do 3 labs 28, 30, and 33 - Practice: + Create IAM policy and role + Create Restriction Policy and IAM limited user + Create key management service,AWS CloudTrail and Amazon Athena + Share encrypted data on S3 10/02/2025 10/02/2025 https://000028.awsstudygroup.com/ https://000030.awsstudygroup.com/ https://000033.awsstudygroup.com/ 6 - Do 2 labs 44 and 48 - Practice: + Create IAM Group, IAM User + Configure role conditon + Access to the application through accesskey and IAM role on EC2 10/03/2025 10/03/2025 https://000044.awsstudygroup.com/ https://000048.awsstudygroup.com/ Week 4 Achievements: Understood about security on AWS:\nShared Responsibility Model AWS Identity and Access Mangement Amazon Cognito AWS Organization KMS Successfully created and configured IAM role and policy.\nBecame familiar with using tags and group resources for managing resources\n"},{"uri":"https://Son2110.github.io/fcj-workshop/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Review knowledges about VPC, EC2, S3, \u0026hellip; Translate 3 blogs Know about minimax algorithm Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review knowledges from module 1 to module 5 + VPC + EC2 + S3 + Security + \u0026hellip; 10/06/2025 10/06/2025 3 - Translate 3 blogs + Rox accelerates sales productivity with AI agents powered by Amazon Bedrock + How Laravel Nightwatch handles billions of observability events in real time with Amazon MSK and ClickHouse Cloud + How to export to Amazon S3 Tables by using AWS Step Functions Distributed Map 10/07/2025 10/07/2025 4 - Review about React + Component + Props + React hook - Learn about minimax algorithms - Practice + Create simple tic tac toe game 10/08/2025 10/08/2025 https://coderschool.notion.site/Naver-AI-Hackathon-Week-1-Web-Minimax-284ea86d567a80f98983c7c219c4308b 5 - Create tic-tac-toe game for player play with AI with 2 mode easy and hard 10/09/2025 10/09/2025 https://coderschool.notion.site/Week-1-Assignment-React-27fea86d567a80718ce4ee04b19ed8d9 6 - Continue fix bugs and optimize tic-tac-toe game - Edit blogs 10/10/2025 10/10/2025 https://github.com/Son2110/tic-tac-toe-ai Week 5 Achievements: Review knowledges about VPC, EC2, S3, and Security, \u0026hellip;\nCompletely translate 3 blogs\nUnderstand about minimax algorithm\nSuccessfully create tic-tac-toe app for player play with AI with 2 mode easy and hard base on minimax algorithm\n"},{"uri":"https://Son2110.github.io/fcj-workshop/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Understand about database concept of AWS Know how to create DB and connect DB to EC2 Complete assignment of hackathon Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about database on AWS + Amazon RDS + Amazon Aurora + Amazon Redshift + Amazon Elasticache 10/13/2025 10/13/2025 https://www.youtube.com/watch?v=OOD2RwWuLRw\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=217 3 - Do labs 05 - Practice: + Create EC2 and RDS + Deploy application on EC2 and connect to RDS + Know backup and restore 10/14/2025 10/14/2025 https://000005.awsstudygroup.com/ 4 - Learn about distributed System + Review network: TCP, HTTP; + Websocket 10/15/2025 10/15/2025 https://coderschool.notion.site/Naver-AI-Hackathon-Week-2-Web-Distributed-Systems-28bea86d567a80f0acf9e624986dcdf6 5 - Practice: + Create multiplayer odd/even tic-tac-toe game 10/16/2025 10/16/2025 https://coderschool.notion.site/Week-2-Assignment-React-28bea86d567a8044bd23fd69e3d9b8df 6 - Practice: + Continue fix bugs and complete game 10/17/2025 10/17/2025 https://github.com/Son2110/tic-tac-toe-even-odd Week 6 Achievements: Understood fundamental database concepts:\nRDBMS and NOSQL OLTP and OLAP Amazon RDS and Aurora Amazon Redshift Amazon ElastiCache Successfully created an configured an Amazon RDS instance.\nLearned to connect to and interact with remote databases using SQL clients or command-line tools from an EC2 instance.\nCreate successfully odd/even tic-tac-toe game\n"},{"uri":"https://Son2110.github.io/fcj-workshop/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Complete labs of module 7 Complete assignment of Hackathon Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review database concepts knowledge 10/20/2025 10/20/2025 3 - Learn about Agile + Scrum + Kanban - Practice - Write product vison and pitch 10/21/2025 10/21/2025 https://coderschool.notion.site/Naver-AI-Hackathon-Week-3-From-Idea-to-Impact-292ea86d567a80779621c45b102205e7 4 - Do 2 labs 35 and 40 - Practice: + Use AWS Glue to scan data in S3 + Connect Amazon Athena to AWS Glue + Connect Amazon QuickSight to Amazon Athena 10/22/2025 10/22/2025 https://000035.awsstudygroup.com/ https://000040.awsstudygroup.com/ 5 - Do 2 labs 60 and 70 - Practice: + Use SDK to create table, CRUD data in Amazon DynamoDB + Build a datalake with data stored in S3 10/23/2025 10/23/2025 https://000060.awsstudygroup.com/ https://000070.awsstudygroup.com/ 6 - Do 2 labs 72 and 73 - Practice: + Use AWS Glue to visually profile, clean and transfrom raw data without writing code + Build dashboard via QuickSight 10/24/2025 10/24/2025 https://000072.awsstudygroup.com/ https://000073.awsstudygroup.com/ Week 7 Achievements: Successfully created product vison and pitch.\nUnderstood the architecture of a Serverless Data Lake and Analytics services:\nAWS Glue Amazon Athena Amazon QuickSight Designed and published interactive dashboards using Amazon QuickSight to visualize data insights and support business decision-making\nUsed Amazon Athena to analyze data directly from S3 using standard SQL for cost analysis and performance monitoring.\n"},{"uri":"https://Son2110.github.io/fcj-workshop/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Review knowledge from module 1 to 7 Learn about Naver Clova Studio Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review knowledge about AWS + Cost Optimization + EC2 basic 10/27/2025 10/27/2025 3 - Learn about Naver Clova Studio - Explore about idea for Hackathon - Review about EC2 10/28/2025 10/28/2025 https://coderschool.notion.site/Naver-AI-Hackathon-Week-4-Web-Naver-AI-Clova-Studio-29bea86d567a80d6abdeeb0cef37d6c0 4 - Review S3 and Shared Responsibility Model 10/29/2025 10/29/2025 5 - Review data concepts - Do some exams from Notebooklm 10/30/2025 10/30/2025 6 - Do exams on Notebooklm - Review all knowledge from module 1 to 7 10/31/2025 10/31/2025 Week 8 Achievements: Understood about Naver Clova Studio\nSuccessfully choose idea for Hackathon is about an app for supporting deaf person to commute with others\nReview all knowledge from module 1 to 7.\n"},{"uri":"https://Son2110.github.io/fcj-workshop/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Build demo web Build connect S3 and Cloudfront for workshop Discover AI model for Hackathon project Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Create basic app, deploy fe on S3 and Cloudfront 11/03/2025 11/03/2025 3 - Create API gateway and lambda function for login logout - Fix bugs about CORS 11/04/2025 11/04/2025 4 - Discover about model AI - Learn how to run this model 11/05/2025 11/05/2025 https://github.com/Etdihatthoc/Multi-VSL_WACV_2025/ 5 - Create a simple web with camera for project - Discover about new model VSL 11/06/2025 11/06/2025 https://github.com/photienanh/Vietnamese-Sign-Language-Recognition 6 - Implement model AI VSL into project 11/07/2025 11/07/2025 Week 9 Achievements: Successfully build simple web for workshop and hackathon\nSuccessfully connect S3 and Cloudfront to deploy fe\nSuccessfuly implement model AI VSL into web\n"},{"uri":"https://Son2110.github.io/fcj-workshop/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Doing task A\u0026hellip;\nWeek 3: Doing task B\u0026hellip;\nWeek 4: Doing task C\u0026hellip;\nWeek 5: Doing task D\u0026hellip;\nWeek 6: Doing task E\u0026hellip;\nWeek 7: Doing task G\u0026hellip;\nWeek 8: Doing task H\u0026hellip;\nWeek 9: Doing task I\u0026hellip;\nWeek 10: Doing task L\u0026hellip;\nWeek 11: Doing task M\u0026hellip;\nWeek 12: Doing task N\u0026hellip;\nWeek 13: Doing task O\u0026hellip;\n"},{"uri":"https://Son2110.github.io/fcj-workshop/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Serverless \u0026amp; Event-Driven Architecture Serverless Architecture: This workshop utilizes a cloud-native model with services like AWS Lambda, Amazon API Gateway, and Amazon DynamoDB. This approach allows code to run in response to requests without provisioning or managing servers, as AWS handles all automatic scaling and infrastructure management. Event-Driven Architecture: The core of the system functions on an event-driven basis. Instead of services continuously polling for data, specific events—such as IoT sensor readings or user API calls—trigger downstream workflows. This is orchestrated by AWS IoT Core and Amazon EventBridge, creating a highly flexible and scalable system. Workshop overview In this workshop, you will deploy a comprehensive serverless data platform on AWS to manage real-time environmental monitoring for an 8-room smart office setup. The system integrates AWS IoT Core, Lambda, DynamoDB, S3, CloudFront, and Amazon Cognito. Sensor data is forwarded from edge devices (or simulated scripts), ingested into AWS, stored in DynamoDB tables, and processed by Lambda functions to update the management dashboard. Critical events are routed through EventBridge for alerting, demonstrating a high-availability, low-cost, and seamless scalability architecture.\n"},{"uri":"https://Son2110.github.io/fcj-workshop/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS\u0026rdquo; Event Objectives Understand the basic concepts of Foundation Models. Learn Prompting techniques to optimize output results. Explore the capabilities of Generative AI through the Amazon Bedrock service. Learn about Retrieval-Augmented Generation (RAG) architecture and how to integrate a Knowledge Base. Speakers Danh Hoang Hieu Nghi Lam Truong Kiet Dinh Le Hoang Anh Key Highlights Prompting Techniques Prompt Optimization: Understood the importance of drafting clear and specific prompts to ensure the AI model returns more accurate results. Chain of Thought: A method of guiding the AI to process problems step-by-step, helping to improve logical reasoning capabilities. RAG (Retrieval-Augmented Generation) Expanding AI Capabilities: Understood how to combine AI with the enterprise\u0026rsquo;s internal data sources instead of relying solely on pre-trained data. Accuracy and Real-time: RAG helps minimize \u0026ldquo;hallucinations\u0026rdquo; (AI fabricating information) and ensures answers are always updated with the latest information. Vector Embeddings Data Representation: How to convert text into numerical vectors so computers can understand semantics and relationships between words. Capturing Context: Embedding models help the system understand the nuances of the text. Multilingual: The ability to compare and search for semantic similarities across different languages. AWS AI Services Amazon Rekognition: Image and video analysis. Amazon Translate: Automatic translation. Amazon Textract: Data extraction from scanned documents. Amazon Transcribe: Speech-to-text conversion. Amazon Polly: Text-to-speech conversion with natural voices. Amazon Comprehend: Text and sentiment analysis. Amazon Kendra: Intelligent search engine for enterprises. Amazon Lookout: Anomaly detection in data. Amazon Personalize: Personalized recommendation system. Key Takeaways Improving Prompting Skills Providing Context: Giving specific examples (few-shot prompting) helps the AI understand the request better. Detailed Instructions: Explaining the processing workflow step-by-step to the AI to solve problems. RAG Applications Minimizing Errors: Connecting to the enterprise\u0026rsquo;s verified knowledge sources. Enhancing Chatbots: Integrating real-time data to make Chatbots respond more intelligently. Personalized Search: Based on user history and personas. Data Summarization: Extracting information from transactional documents. Amazon Titan Embeddings Semantic Search: Allows searching based on the meaning of keywords rather than just character matching. Performance: Supports processing large token counts and diverse languages. Plan for Application Further Research: Continue to explore AI services more deeply to evaluate their applicability for future projects. Experimentation: Start practicing the prompting techniques learned to improve work efficiency. Event Experience Attending the “AI/ML/GenAI on AWS” workshop was a valuable opportunity that helped me take the initial steps into the field of cloud-based AI. Here are my main experiences:\nLearning from Experts Speakers from FACJ shared practical best practices regarding AI deployment, helping me visualize the process of applying it in reality more clearly. Through specific examples, I gained a better understanding of how prompting techniques operate. Practical Discovery of Services Watching live demos helped me understand the operating mechanisms of AWS AI services and how they solve real-life problems. Guidance on Building Agents Acquired the necessary foundational knowledge to start exploring how to build AI Agents. Some event photos Overall, the event equipped me with important foundational knowledge about Generative AI, setting the premise for me to research and apply it to projects in the future.\n"},{"uri":"https://Son2110.github.io/fcj-workshop/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Change project into gitlab Configure CI/CD for project Discover how to translate continously words with model AI Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Find way that how to model can translate long video with a lot of gestures 11/10/2025 11/10/2025 3 - Change project from Github to Gitlab - Learn how to use Gitlab - Configure CI/CD 11/11/2025 11/11/2025 https://gitlab.com/son2110-group/AWS-Project 4 - Implement sliding window in be logic to translate continously words 08/13/2025 08/13/2025 https://chatgpt.com/s/t_6911e6be651081918559f768ba18a0be 5 - Fix bugs and test web 08/14/2025 08/14/2025 6 - Implement paused-based segmentation in be 08/15/2025 08/15/2025 Week 10 Achievements: Successfully configure CI/CD for gitlab to S3 and Cloudfront\nImplement two ways about translate continuous words into web\nTest accuracy of two ways\n"},{"uri":"https://Son2110.github.io/fcj-workshop/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Build MVP Hackathon project. Create API gateway and lambda function for workshop Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Participate in AWS Cloud Mastery Series #2 11/17/2025 11/17/2025 3 - Create forget password function for web - Fix bugs lambda and CORS 11/18/2025 11/18/2025 4 - Implement split 3s for video on fe and testing 11/19/2025 11/19/2025 5 - Implement Web Speech API in project and continue testing - Learn Coursera 11/20/2025 11/20/2025 https://www.coursera.org/learn/research-methods 6 - Enhance UI of web for function record video and split 3s - Learn Coursera 11/21/2025 11/21/2025 https://www.coursera.org/learn/research-methodologies Week 11 Achievements: Successfully create forget password for workshop\nSuccessfully create TalkSign project for hackathon\n"},{"uri":"https://Son2110.github.io/fcj-workshop/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"a\nWeek 12 Objectives: Completely present at hackathon demo day\nContinue to complete the workshop\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn Coursera - Prepare for hackathon presentation 11/24/2025 11/24/2025 https://www.coursera.org/learn/introduction-to-research-for-essay-writing?specialization=academic-english 3 - Present online at hackathon demo day - Create function and connect API to web + RoomList + RoomConfig 11/25/2025 11/25/2025 https://drive.google.com/file/d/1qZgxEglgt08jQaZD7WQv0CcijgbpNH53/view?usp=drive_link 4 - Create landing page - Change UI for project - Create function UserList 11/26/2025 11/26/2025 5 - Learn about deploy github page - Redesign the flow of project 11/27/2025 11/27/2025 6 - Implement new admin dashboard and sign up 11/28/2025 11/28/2025 Week 12 Achievements: Successfully present at hackathon demo day\nSuccessfully redesign the flow of project\nSuccessfully implement new admin dashboard and sign up\n"},{"uri":"https://Son2110.github.io/fcj-workshop/1-worklog/1.13-week13/","title":"Week 13 Worklog","tags":[],"description":"","content":"Week 13 Objectives: Completely workshop Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Redo login lambda and web design for login with two type of accounts(Admin, manager) - Create CRUD for admin account 12/1/2025 12/1/2025 3 - Fix bug login for manager - Fix 2 function ListRoom and UpdateRoom - Create chart for data - Create new create room with IoT core 12/2/2025 12/2/2025 4 - Implement a virtual IoT device on my local laptop to receive and send data to AWS 12/3/2025 12/3/2025 5 - Fix some bug in UI and script python for emulating an IoT device 12/4/2025 12/4/2025 6 - Test and optimize web 12/5/2025 12/5/2025 Week 13 Achievements: Implemented a Python script to simulate a local IoT device, successfully establishing bidirectional MQTT communication (Telemetry \u0026amp; Config) with AWS IoT Core.\nCompletely the workshop\n"},{"uri":"https://Son2110.github.io/fcj-workshop/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"Create IAM User for this workshop In AWS Management Console, search and choose IAM Navigate to User, click Create user For User name, enter admin-user Check Provide user access to the AWS Management Console - optional For Console password, check Custom password Enter password for your user Uncheck Users must create a new password at next sign-in - Recommended for easy operation Click Next For Permissions options, check Attach policies directly Click Create policy You will be directed to Create policy page For Policy editor, switch to JSON Copy and paste this policy { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;InfrastructureManagement\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;iam:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;BackendComputeAndAPI\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;lambda:*\u0026#34;, \u0026#34;apigateway:*\u0026#34;, \u0026#34;execute-api:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;DatabaseAndAuth\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:*\u0026#34;, \u0026#34;cognito-idp:*\u0026#34;, \u0026#34;cognito-identity:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;IoTServices\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iot:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;StorageAndHosting\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:*\u0026#34;, \u0026#34;cloudfront:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;MonitoringAndLogging\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;events:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Click Next For Policy name, enter your policy name (E.g. SmartOfficeAdminFullAcccess) Add Tags for cost and operation management (Key: Project, Value: SmartOffice; Key: Environment, Value: Dev) Click Create policy Go back to Step 2 Set permissions of Create user Search and choose your policy name Click Next Add Tags for cost and operation management (Key: Project, Value: SmartOffice; Key: Environment, Value: Dev) Click Create user Login with your User account to begin this workhop "},{"uri":"https://Son2110.github.io/fcj-workshop/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Smart Office Management System for Lab Research A Unified AWS Serverless Solution for Real-Time Monitoring \u0026amp; Administration 1. Executive Summary The Smart Office Management System is proposed by Team Skyscraper from FPTU HCM Campus, inspired by the operational excellence observed during a field trip to the AWS office in Ho Chi Minh City. Traditional office management lacks real-time visibility into room conditions (temperature, humidity, light) and relies heavily on manual oversight. To address this, we propose a centralized Management Console built on a fully AWS Serverless architecture. By leveraging services like AWS IoT Core, Lambda, and DynamoDB, the system collects sensor data every 2-5 minutes to support real-time monitoring and allows administrators to manage device configurations remotely. This project also serves as a strategic \u0026ldquo;First Cloud AI Journey\u0026rdquo;, enabling the team to bridge the gap between theoretical knowledge and practical application of Cloud Computing.\n2. Problem Statement What’s the Problem? Nowadays, managing office environments in research labs requires manual intervention to check device statuses (lights, air conditioners) and environmental conditions. Managers often lack the data needed to make informed decisions about energy usage or room comfort. Operating devices on fixed schedules (e.g., 8 a.m. to 5 p.m.) without regarding actual room usage or environmental factors leads to energy waste. Furthermore, without a centralized dashboard, administrators cannot quickly detect anomalies or configure settings for multiple rooms efficiently.\nThe Solution The platform uses AWS IoT Core to ingest MQTT data from room sensors, AWS Lambda and API Gateway for backend logic and processing, Amazon DynamoDB for storing sensor logs and room configurations, and Amazon S3 combined with CloudFront to host the web management dashboard. Access is strictly secured via Amazon Cognito. Amazon EventBridge is utilized to handle scheduled automation tasks, while Amazon SNS ensures timely notifications for system alerts. This solution replaces manual tracking with a digital, real-time management console capable of monitoring multiple rooms simultaneously.\nBenefits and Return on Investment The Smart Office Management System enhances operational efficiency by providing a single pane of glass for monitoring and configuration. It empowers lab managers to control devices remotely and make data-driven decisions. Beyond operational improvements, the project provides a reusable serverless foundation for future IoT research at the university.\nMonthly operating costs are estimated at $1.81 USD, utilizing the AWS Free Tier for services like Lambda, API Gateway, and DynamoDB. Major costs include CloudFront ($1.27) and CloudWatch ($0.25), totaling approximately $21.72 USD per year. Since the system leverages existing ESP32 hardware and sensors, there are no additional capital expenditures. The system offers immediate value through time savings and reduced management effort.\n3. Solution Architecture The Smart Office system adopts a fully serverless AWS architecture optimized for cost efficiency and scalability. Data from multiple sensor hubs is transmitted to AWS IoT Core, processed by Lambda functions, and stored in DynamoDB for real-time monitoring and configuration management. EventBridge automates scheduled device actions, while SNS handles system notifications. The web dashboard is hosted on S3 and delivered securely via CloudFront, with user authentication managed through Amazon Cognito. This architecture minimizes operational overhead and ensures high reliability for smart environment control.\nAWS Services Used AWS IoT Core: Ingests and manages MQTT data from smart room hubs, enabling secure communication between edge devices and the cloud. AWS Lambda: Executes backend logic for processing sensor telemetry, handling API requests, and executing management commands (Serverless Compute). Amazon API Gateway: Exposes secure RESTful endpoints for the web dashboard to interact with the backend services. Amazon DynamoDB: Provides fast, NoSQL storage for room configurations, device states, and historical sensor logs. Amazon EventBridge: Orchestrates event-driven workflows, such as scheduled configuration updates or heartbeat checks. Amazon SNS: Sends email notifications to administrators regarding system alerts or critical updates. Amazon S3: Hosts the frontend static assets (HTML, CSS, JS) for the Management Dashboard. Amazon CloudFront: Delivers the web application globally with low latency and SSL security. Amazon Cognito: Manages user identity, authentication, and access control for the Management Console. Amazon CloudWatch: Collects logs and metrics to monitor system health and debug Lambda executions. Component Design Sensor Hubs: IoT-enabled devices (ESP32) in each room collect telemetry (temperature, humidity, light) and transmit it to AWS IoT Core every few minutes. Data Ingestion: AWS IoT Core rules trigger the HandleTelemetry Lambda, which validates data and persists it to Amazon DynamoDB. Configuration Management: Administrators use the dashboard to update room settings. The RoomConfigHandler Lambda updates DynamoDB and pushes changes to devices via IoT Core Shadows or MQTT. User Interaction: The Web Dashboard (on S3/CloudFront) visualizes real-time data and provides a control interface. User Authentication: Amazon Cognito ensures only authorized lab members can log in and access sensitive room data. Monitoring \u0026amp; Reliability: Amazon CloudWatch tracks system performance, ensuring high availability and rapid troubleshooting. 4. Technical Implementation Implementation Phases\nResearch \u0026amp; Foundation (Weeks 1-7): Study core AWS services (IoT Core, Lambda, DynamoDB, S3, API Gateway, Cognito) and understand Serverless design patterns. Architecture Design \u0026amp; Estimation (Week 8): Finalize the solution diagram for an 8-room setup and use the AWS Pricing Calculator to forecast the budget. Development (Weeks 9-11): Implement firmware/scripts for IoT data simulation. Develop Backend: Lambda functions, DynamoDB tables, and API Gateway resources using CloudFormation/CDK. Develop Frontend: Build the Management Dashboard and integrate with APIs. Testing \u0026amp; Deployment (Week 12): Perform end-to-end testing, validate data flow from sensors to the dashboard, and deploy the system to the production environment. Technical Requirements\nHardware Layer: ESP32-based Sensor Hubs monitoring environmental metrics. Cloud Layer: A fully serverless stack on AWS (IoT Core, Lambda, DynamoDB, API Gateway, S3, CloudFront, Cognito, EventBridge, SNS). DevOps: Infrastructure as Code (IaC) using AWS CloudFormation for reproducible deployments. Interface: A responsive web dashboard allowing real-time monitoring and configuration updates. 5. Timeline \u0026amp; Milestones Project Timeline\nWeeks 1–7: Deep dive into AWS services and complete \u0026ldquo;First Cloud AI Journey\u0026rdquo; fundamental training. Week 8: Design the system architecture and finalize cost estimation. Weeks 9–11: Core development phase (Backend logic, Database schema, Frontend UI integration). Week 12: System integration testing, debugging, and final Go-Live presentation. 6. Budget Estimation You can find the budget estimation on the AWS Pricing Calculator.\nOr you can download the Budget Estimation File.\nInfrastructure Costs AWS Services:\nAmazon DynamoDB: Free (Always Free Tier: 25GB Storage). AWS Lambda: Free (Always Free Tier: 1M requests/month). AWS IoT Core: $0.18/month (8 devices, sending data every 2 mins). Amazon API Gateway: Free (Free Tier: 1M calls/month for 12 months). Amazon S3: Free (Standard storage \u0026lt; 5GB). Amazon CloudFront: $1.27/month (Based on est. data transfer and requests). Amazon EventBridge: Free (Free Tier events). Amazon SNS: $0.02/month (Email notifications). Amazon CloudWatch: $0.25/month (Log ingestion and storage). Amazon Cognito: Free (Free Tier: 50,000 MAUs). Hardware: Mock script Total: ≈ $1.81/month, or $21.72/year (optimized within AWS Free Tier).\n7. Risk Assessment Risk Matrix IoT Connectivity Issues: Medium impact, medium probability. Sensor Data Inaccuracy: Medium impact, low probability. Unexpected AWS Charges: Low impact, low probability (due to strict budget alerts). Security Misconfiguration: High impact, low probability. Mitigation Strategies Connectivity: Implement retry logic on edge devices and local buffering. Cost: Configure AWS Budgets to alert when spending exceeds $5.00. Security: Enforce strict IAM policies (Least Privilege) and require authentication for all API access via Cognito. Reliability: Use CloudWatch Logs to trace errors in Lambda execution immediately. Contingency Plans Enable manual override controls if the cloud system becomes unavailable. Maintain a backup of the CloudFormation templates for rapid redeployment. 8. Expected Outcomes Technical Improvements: Replaces manual checks with real-time digital monitoring. Provides a centralized platform for managing configurations across multiple rooms. Establishes a scalable architecture capable of supporting more devices in the future. Long-term Value: Serves as a practical learning hub for students to master AWS Serverless technologies. Provides data insights that can lead to better energy usage policies in the lab. Demonstrates a cost-effective, production-ready cloud solution. Proposal Link Smart_Office_Proposal\n"},{"uri":"https://Son2110.github.io/fcj-workshop/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS Cloud Mastery Series #2: DevOps on AWS\u0026rdquo; Event Objectives Understand what the \u0026ldquo;DevOps Mindset\u0026rdquo; actually is. Get to know AWS services for DevOps and CI/CD pipelines. Learn about \u0026ldquo;Infrastructure as Code\u0026rdquo; (IaC) - controlling things with code. Explore Container services on AWS. How to check if the system is healthy (Monitoring \u0026amp; Observability). Hear real-world tips and \u0026ldquo;battle stories\u0026rdquo; from the experts. Speakers Truong Quang Tinh Nghiem Le Long Huynh Quy Pham Key Highlights DevOps Mindset Collaboration is key: Devs and Ops need to work together and share responsibility, not blame each other. Automate everything: If you have to do it twice, write a script. Don\u0026rsquo;t do it by hand. Keep learning: Tech changes fast, so we have to keep learning and trying new things. Measurement: You need data/numbers to know if you are doing well or not. The DevOps Journey (For beginners) Do:\nStart with the basics (Linux, Network\u0026hellip;). Learn by doing: Build real projects yourself. Document everything you do. Master one thing at a time, don\u0026rsquo;t rush. Improve soft skills (communication). Don\u0026rsquo;t:\nStay in \u0026ldquo;tutorial hell\u0026rdquo; (watching videos without coding). Blindly copy-paste code without understanding how it runs. Compare your progress to others. Give up when you hit a bug. CI/CD Pipeline Understanding the app lifecycle: From coding, testing, reviewing, staging (pre-prod), all the way to real users (production). Infrastructure as Code (IaC) The concept: Use code to manage cloud resources instead of clicking around in the console manually. Automation: Automatically create, update, and delete infrastructure to avoid mistakes. Tools: Terraform, OpenTofu, Pulumi. Container Services on AWS Managing containers with Docker, Kubernetes, Amazon ECR, and Amazon EKS. Amazon App Runner: A really cool service to deploy web apps directly from code without worrying about servers or complex configs (great for newbies). Monitoring \u0026amp; Observability Using best practices with Amazon CloudWatch and Amazon X-Ray to see if the system is running smoothly. Key Takeaways DevOps metrics that matter Tracking deployment health. Making the system more flexible and stable. Ensuring the best user experience. Proving that the tech we use is worth the investment. The \u0026ldquo;Continuous\u0026rdquo; in CI/CD Seeing the full picture of how a CI/CD pipeline flows from start to finish. IaC in AWS Knowing how to use Amazon CloudFormation to create templates for infrastructure, so we don\u0026rsquo;t have to build it manually every time. Applying to Work Build a roadmap: Plan my own career path in DevOps. Try CI/CD: Attempt to add an automated pipeline to my current project (stop manual deployments). Make Templates: Write infrastructure code templates to reuse later and reduce human errors. Event Experience Attending the “DevOps on AWS” workshop was really eye-opening, almost like having a guide for my DevOps career path. Some memorable experiences included:\nLearning from the Speakers Experts from FACJ shared very honest stories about their daily DevOps work. Got to see a live demo of system monitoring, which looked really professional. Exploring CI/CD and IaC Understood how big companies update their software continuously using CI/CD pipelines. Learned how to write code to build infrastructure (using CloudFormation/CDK) instead of tweaking things by hand. System Monitoring Learned more about setting up alerts and viewing dashboards, as well as the on-call process for handling incidents. Some event photos Overall, this event gave me a full picture of the DevOps career. More importantly, I picked up some \u0026ldquo;best practices\u0026rdquo; on how to apply CI/CD and monitoring properly.\n"},{"uri":"https://Son2110.github.io/fcj-workshop/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - Rox accelerates sales productivity with AI agents powered by Amazon Bedrock This article introduces how Rox utilizes Amazon Bedrock to build a \u0026ldquo;Revenue Operating System.\u0026rdquo; You will learn how AI Agents help automate complex sales processes—from customer research and contact discovery to drafting proposals—enabling sales teams to increase productivity and focus on high-value opportunities.\nBlog 2 - How Laravel Nightwatch handles billions of observability events in real time This blog delves into the technical architecture of Laravel Nightwatch, an application performance monitoring platform. It describes how they combine Amazon MSK and ClickHouse Cloud to build a scalable streaming pipeline capable of processing billions of events daily with sub-second latency while ensuring cost efficiency.\nBlog 3 - How to export to Amazon S3 Tables by using AWS Step Functions Distributed Map This is a detailed technical guide on building a serverless ETL workflow. The article demonstrates how to use AWS Step Functions Distributed Map to process large volumes of data in parallel (e.g., extracting information from PDF files using Textract) and export structured data directly to Amazon S3 Tables in Apache Iceberg format for analysis.\n"},{"uri":"https://Son2110.github.io/fcj-workshop/5-workshop/5.3-run-cloudformation-stack/","title":"Set up cloudformation","tags":[],"description":"","content":"Download resources Download these CloudFormation template files:\nsmart_office_budget.yaml smart_office_s3_cloudfront.yaml smart_office_cognito.yaml smart_office_dynamodb.yaml smart_office_lambda_authenticate_with_dynamodb_cognito.yaml smart_office_lambda_readonly_with_dynamodb.yaml smart_office_lambda_crud_with_dynamodb_cognito.yaml smart_office_lambda_crud_with_dynamodb_iot.yaml smart_office_iot_core.yaml smart_office_api_gateway.yaml Deploy CloudFormation Stacks In AWS Management Console, search and choose CloudFormation Click Create stack For Prepare template, check Choose an existing template For Template source, check Upload a template file Click Choose file Choose file smart_office_budget.yaml Click Next For Stack name, enter SmartOffice-Budget-Dev Click Next Add Tags for cost and operation management (Key: Project, Value: SmartOffice; Key: Environment, Value: Dev) For Stack failure options, check Preserve successfully provisioned resources (To keep created resource for debugging) Click Next Check again and click Submit Do the same for other files with exactly name Template name Stack name smart_office_s3_cloudfront.yaml SmartOffice-S3-CloudFront-Dev smart_office_cognito.yaml SmartOffice-Cognito-Dev smart_office_dynamodb.yaml SmartOffice-DynamoDB-Dev smart_office_lambda_authenticate_with_dynamodb_cognito.yaml SmartOffice-Authenticate-Lambda-Dev smart_office_lambda_readonly_with_dynamodb.yaml SmartOffice-ReadOnly-Lambda-Dev smart_office_lambda_crud_with_dynamodb_cognito.yaml SmartOffice-Crud-Lambda-Dev smart_office_lambda_crud_with_dynamodb_iot.yaml SmartOffice-IoT-Lambda-Dev smart_office_iot_core.yaml SmartOffice-IoT-Core-Dev smart_office_api_gateway.yaml SmartOffice-API-Gateway-Dev "},{"uri":"https://Son2110.github.io/fcj-workshop/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS Cloud Mastery Series #3: AWS Well-Architected Security Pillar\u0026rdquo; Event Objectives Understanding the basics of Security (The Foundation). Controlling who gets in and what they can do (Identity \u0026amp; Access Management). Spotting the bad guys (Detection). Protecting the servers and the data (Infrastructure \u0026amp; Data Protection). What to do when things go wrong (Incident Response). Speakers Le Vu Xuan An Tran Duc Anh Tran Doan Cong Ly Danh Hoang Hieu Nghi Thinh Lam Viet Nguyen Mendel Branski (Long) Key Highlights Introduction to Cloud Clubs Got to know about Cloud Clubs at universities like UTE and SGU. It\u0026rsquo;s cool to see student communities active in this field. Security Foundation SCPs: The big rules for the whole organization. Permission Boundaries: Setting limits so users can\u0026rsquo;t give themselves too much power. MFA: The absolute must-have for security. Detection and Monitoring Multi-Layer Security: Checking security at every level, not just the front door. Alerting: Using EventBridge to wake us up when something suspicious happens. Detection-as-Code: Writing code to automatically find threats (sounds fancy and super useful). Incident Response Prevention: The best incident is the one that never happens. \u0026ldquo;Sleeping better\u0026rdquo;: Setting things up so you don\u0026rsquo;t get paged at 3 AM. Learning the step-by-step process of fixing things when they break. Key Takeaways Service Control Policies (SCPs) Think of this as the \u0026ldquo;Constitution\u0026rdquo; for your AWS accounts. It controls the maximum permission available. Even if you are an Admin, if the SCP says \u0026ldquo;No\u0026rdquo;, it means No. It never grants permission; it only filters/blocks things. Permission Boundaries A way to stop a user (or a role) from becoming too powerful. It sets a \u0026ldquo;ceiling\u0026rdquo; on what they can ever do, no matter what other policies say. Multi-Factor Authentication (MFA) - Just do it! TOTP: The standard Google Authenticator codes we use. It\u0026rsquo;s free and works well. FIDO2: The physical USB keys (like YubiKey). More secure, requires a touch, harder to hack. Detection-as-Code Using tools like CloudFormation/Terraform to automatically set up GuardDuty (the threat detector) everywhere. Version Control: Storing your security rules in Git so you can track changes just like software code. Incident Response Prepare: Have automation ready before the hack happens. Lesson Learned: After fixing the issue, always look back to see why it happened so it doesn\u0026rsquo;t happen again. Applying to Work Least Privilege: Go back and check permissions. Don\u0026rsquo;t give \u0026ldquo;AdminAccess\u0026rdquo; to everyone anymore. Enforce MFA: Make sure every account uses MFA. Plan ahead: Think about \u0026ldquo;What if this breaks?\u0026rdquo; and have a plan ready. Event Experience Attending the “AWS Well-Architected Security Pillar” workshop helped me realize that security isn\u0026rsquo;t just about firewalls; it\u0026rsquo;s a whole mindset.\nLearning from the Speaker Listening to speaker talk about how they handle real incidents was eye-opening. They focused a lot on \u0026ldquo;lessons learned\u0026rdquo; rather than just fixing the bug. Networking with Cloud Clubs It was nice to see the energy from the university Cloud Clubs. It connects learners like me from everywhere. Real-time Defense I learned that we can\u0026rsquo;t watch screens 24/7. We need tools like CloudTrail and EventBridge to watch the system for us and alert us immediately. Overall, this event expanded my view on Automation and Security. It\u0026rsquo;s not just about locking doors; it\u0026rsquo;s about having a smart alarm system and knowing what to do when it goes off.\n"},{"uri":"https://Son2110.github.io/fcj-workshop/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"Event 1 Event Name: Vietnam Cloud Day HCMC Connect Edition (GenAI \u0026amp; Data Track)\nDate \u0026amp; Time: 08:30, September 26, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nDescription: This was a major event about cloud technology, and I chose to join the track focused on GenAI and Data. Speakers from AWS and partners (like TymeX) shared the importance of building a \u0026ldquo;Unified Data Foundation\u0026rdquo; to feed AI. I learned about breaking down data \u0026ldquo;silos\u0026rdquo; (isolated storage) and using Amazon Bedrock to build AI apps quickly without worrying too much about the complex infrastructure underneath.\nOutcome: I really internalized the idea that \u0026ldquo;Data is fuel for AI\u0026rdquo; – if the data isn\u0026rsquo;t clean and organized, the AI won\u0026rsquo;t be smart. I learned about the Zero-ETL trend, which helps reduce the tiring work of moving data around manually. The real-world case study from TymeX in the finance sector gave me confidence that GenAI isn\u0026rsquo;t just a trend; it actually solves real business problems.\nEvent 2 Event Name: AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS\nDate \u0026amp; Time: 08:30, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nDescription: An introduction to the big AI models (Foundation Models) and Generative AI using Amazon Bedrock. The speakers explained how to write good prompts (using a technique called Chain of Thought) and explained RAG architecture, which helps AI use our own internal data. We also looked at vector embeddings and got an overview of the different AWS AI services.\nOutcome: I learned a lot about how to stop AI from making things up (hallucinations) by using RAG and Embeddings. I learned which AWS AI service to use for different problems. The event gave me some good insights into building AI agents and was a great chance to listen to experts in the field.\nEvent 3 Event Name: AWS Cloud Mastery Series #2: DevOps on AWS\nDate \u0026amp; Time: 08:30, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nDescription: A complete look at the \u0026ldquo;DevOps mindset,\u0026rdquo; focusing on teamwork and automating everything. We covered essential services like CI/CD pipelines, \u0026ldquo;Infrastructure as Code\u0026rdquo; (using code to build servers) with CloudFormation, and container management using EKS. They also shared a helpful \u0026ldquo;Do\u0026rsquo;s and Don\u0026rsquo;ts\u0026rdquo; list for beginners starting a DevOps journey and tips on how to monitor systems properly.\nOutcome: I got a clearer roadmap for a career in DevOps and understood the software life cycle better. I learned why automating things with code (IaC) helps avoid human mistakes. I also gained insights into keeping systems healthy through monitoring, thanks to the demos from the experts.\nEvent 4 Event Name: AWS Cloud Mastery Series #3: AWS Well-Architected Security Pillar\nDate \u0026amp; Time: 08:30, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nDescription: A deep dive into keeping things safe on AWS, covering Identity, Infrastructure, and Data protection. We looked at foundational security steps like permissions and MFA, but also explored advanced strategies like \u0026ldquo;Detection-as-Code\u0026rdquo; to find threats automatically using code and setting up automated responses.\nOutcome: I learned why it\u0026rsquo;s important not to give everyone full access (the principle of least privilege) and how to enforce it effectively. I picked up some practical skills on setting up automatic alerts when something looks suspicious. It really highlighted that in security, prevention is better than cure to minimize risks.\n"},{"uri":"https://Son2110.github.io/fcj-workshop/5-workshop/5.4-set-up-website/","title":"Set up website","tags":[],"description":"","content":"Set up Gitlab repository to deploy website and lambda code Go to this Gitlab repository: https://gitlab.com/tranngockhiet22062005/smart-office Download and deploy on your own repository Set up role for Gitlab to deploy website to S3 and deploy code to Lambda Function Create an IAM User with following attribute (view 5.2 if you forget) User name: smart-office-gitlab-ci Policy: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;S3ListBucketAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:GetBucketLocation\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::fcj-smart-office-frontend-ACCOUNT_ID-dev\u0026#34;, \u0026#34;arn:aws:s3:::fcj-smart-office-lambda-ACCOUNT_ID-dev\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;S3ReadWriteAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:PutObjectAcl\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::fcj-smart-office-frontend-ACCOUNT_ID-dev/*\u0026#34;, \u0026#34;arn:aws:s3:::fcj-smart-office-lambda-ACCOUNT_ID-dev/*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;LambdaUpdateAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;lambda:UpdateFunctionCode\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetFunctionConfiguration\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:lambda:*:*:function:SmartOffice-*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CloudFrontInvalidation\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudfront:CreateInvalidation\u0026#34;, \u0026#34;cloudfront:GetDistribution\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } You should replace ACCOUNT_ID with your AWS Account ID\nPolicy name: SmartOfficeGitlabAccess Navigate to the user In Summary, click Create access key For Use case, check Command Line Interface (CLI) Check I understand the above recommendation and want to proceed to create an access key. Click Next Click Create access key Click Download .csv file Config Gitlab variables Go to your Gitlab repository, click Setting \u0026gt; Variables Click Add variable For each Key and Value follow the table bellow to know where to get value Key Value AWS_ACCESS_KEY_ID In your smart-office-gitlab-ci_accessKeys.csv you have downloaded AWS_DEFAULT_REGION ap-southeast-1 (or anywhere you deploy the workshop) AWS_SECRET_ACCESS_KEY In your smart-office-gitlab-ci_accessKeys.csv you have downloaded CLOUDFRONT_DISTRIBUTION_ID CloudFront \u0026gt; Distributions \u0026gt; Your distribution ID S3_BUCKET_FRONTEND fcj-smart-office-frontend-ACCOUNT_ID-dev (replace ACCOUNT_ID with your AWS Account ID) S3_BUCKET_LAMBDA fcj-smart-office-lambda-ACCOUNT_ID-dev (replace ACCOUNT_ID with your AWS Account ID) STACK_NAME_AUTH SmartOffice-Authenticate-Lambda-Dev STACK_NAME_CRUD SmartOffice-Crud-Lambda-Dev STACK_NAME_IOT SmartOffice-IoT-Lambda-Dev STACK_NAME_READONLY SmartOffice-ReadOnly-Lambda-Dev VITE_API_BASE_URL API Gateway \u0026gt; SmartOffice-API-Gateway-Dev-Api \u0026gt; Stages \u0026gt; Invoke URL Push code into an init branch Merge branchs in this order: init -\u0026gt; dev, dev -\u0026gt; main "},{"uri":"https://Son2110.github.io/fcj-workshop/5-workshop/5.5-event-bridge/","title":"EventBridge and Lambda Setup","tags":[],"description":"","content":"Overview This section will guide you through setting up Amazon EventBridge and Lambda to route and react to events happening in DynamoDB. For SNS setup (used to send alerts), please refer to section 5.6 - SNS Setup.\nCreate AutomationSetup (Lambda + rules) Create a Lambda function (AutomationSetup) whose job is to read automation configuration from DynamoDB and define two EventBridge rules: one to turn automation ON and another to turn it OFF. import boto3 import json import os from boto3.dynamodb.types import TypeDeserializer deserializer = TypeDeserializer() events_client = boto3.client(\u0026#39;events\u0026#39;) HANDLER_ARN = os.environ.get(\u0026#39;HANDLER_LAMBDA_ARN\u0026#39;) def ddb_deserialize(image): d = {} for key in image: d[key] = deserializer.deserialize(image[key]) return d def time_to_cron(time_str): try: hour, minute = map(int, time_str.split(\u0026#39;:\u0026#39;)) utc_hour = hour - 7 if utc_hour \u0026lt; 0: utc_hour += 24 return f\u0026#34;cron({minute} {utc_hour} * * ? *)\u0026#34; except: return None # --- UPDATE 1: Add office_id parameter to function --- def create_or_update_schedule(room_id, office_id, time_str, action): rule_name = f\u0026#34;Room_{room_id}_Auto_{action}\u0026#34; cron_expr = time_to_cron(time_str) if not cron_expr: return print(f\u0026#34;Updating Rule: {rule_name} with Input\u0026#34;) events_client.put_rule( Name=rule_name, ScheduleExpression=cron_expr, State=\u0026#39;ENABLED\u0026#39;, Description=f\u0026#39;Auto {action} for {room_id} in {office_id}\u0026#39; ) # --- UPDATE 2: Add officeId to Input JSON --- target_input = json.dumps({ \u0026#34;roomId\u0026#34;: room_id, \u0026#34;officeId\u0026#34;: office_id, # \u0026lt;--- IMPORTANT: Include officeId \u0026#34;command\u0026#34;: action.upper(), \u0026#34;source\u0026#34;: \u0026#34;Scheduled_Event\u0026#34; }) events_client.put_targets( Rule=rule_name, Targets=[{ \u0026#39;Id\u0026#39;: \u0026#39;1\u0026#39;, \u0026#39;Arn\u0026#39;: HANDLER_ARN, \u0026#39;Input\u0026#39;: target_input }] ) def lambda_handler(event, context): print(\u0026#34;Raw Event:\u0026#34;, json.dumps(event)) if \u0026#39;Records\u0026#39; in event: for record in event[\u0026#39;Records\u0026#39;]: if record[\u0026#39;eventName\u0026#39;] in [\u0026#39;INSERT\u0026#39;, \u0026#39;MODIFY\u0026#39;]: raw_image = record[\u0026#39;dynamodb\u0026#39;][\u0026#39;NewImage\u0026#39;] data = ddb_deserialize(raw_image) room_id = data.get(\u0026#39;roomId\u0026#39;) # --- UPDATE 3: Get officeId from DynamoDB --- office_id = data.get(\u0026#39;officeId\u0026#39;) auto_control = data.get(\u0026#39;autoControl\u0026#39;) auto_on = data.get(\u0026#39;autoOnTime\u0026#39;) auto_off = data.get(\u0026#39;autoOffTime\u0026#39;) if auto_control == \u0026#34;ON\u0026#34; and room_id: # Pass office_id to schedule creation function if auto_on: create_or_update_schedule(room_id, office_id, auto_on, \u0026#39;ON\u0026#39;) if auto_off: create_or_update_schedule(room_id, office_id, auto_off, \u0026#39;OFF\u0026#39;) return {\u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: \u0026#39;Processed\u0026#39;} Go to Configuration \u0026ndash;\u0026gt; Trigger to configure the Lambda triggers — this will be used to invoke the lambda whenever there is any new stream from DynamoDB Select Add Trigger, Select DynamoDB and choose the table that contains rooms\u0026rsquo;s config. In Configuration \u0026ndash;\u0026gt; Environment Variable, add this key-value pair (replace accordingly with your personal information) Select Configuration \u0026ndash;\u0026gt;Role name, and make sure you have those 3 policies: AutomationSetup_RuleExecuiton for creating rules in EventBridge AWSLambdaBasicExecutionRole for basic lambda execution permissions AWSLambdaDynamoDBExecutionRole for interacting with DynamoDB. { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;events:DeleteRule\u0026#34;, \u0026#34;events:PutTargets\u0026#34;, \u0026#34;events:EnableRule\u0026#34;, \u0026#34;events:PutRule\u0026#34;, \u0026#34;events:DisableRule\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Only for AutomationSetup_RuleExecuiton, select Add permissions \u0026ndash;\u0026gt; Create inline policy\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:ap-southeast-1:261899902491:*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:ap-southeast-1:261899902491:log-group:/aws/lambda/AutomationSetup:*\u0026#34; ] } ] } AWSLambdaBasicExecutionRole\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:DescribeStream\u0026#34;, \u0026#34;dynamodb:GetRecords\u0026#34;, \u0026#34;dynamodb:GetShardIterator\u0026#34;, \u0026#34;dynamodb:ListStreams\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } AWSLambdaDynamoDBExecutionRole\nThese two rules correspond to the ON and OFF automation behaviours.\nCreate AutomationHandler (Lambda to forward events to AWS IoT Core) Create the AutomationHandler Lambda to receives events from EventBridge and forwards them to AWS IoT Core. import boto3 import json import logging logger = logging.getLogger() logger.setLevel(logging.INFO) iot_client = boto3.client(\u0026#39;iot-data\u0026#39;, region_name=\u0026#39;ap-southeast-1\u0026#39;) def lambda_handler(event, context): \u0026#34;\u0026#34;\u0026#34; Input from EventBridge: {\u0026#34;roomId\u0026#34;: \u0026#34;test2\u0026#34;, \u0026#34;officeId\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;ON\u0026#34;, ...} \u0026#34;\u0026#34;\u0026#34; # Log entire event to verify payload logger.info(f\u0026#34;Executing Automation: {json.dumps(event)}\u0026#34;) # 1. Extract data from Event room_id = event.get(\u0026#39;roomId\u0026#39;) command = event.get(\u0026#39;command\u0026#39;) # ON / OFF office_id = event.get(\u0026#39;officeId\u0026#39;) # 2. Validate input data if not room_id or not command: logger.error(\u0026#34;Missing roomId or command\u0026#34;) return {\u0026#39;statusCode\u0026#39;: 400, \u0026#39;body\u0026#39;: \u0026#39;Missing roomId or command\u0026#39;} if not office_id: logger.error(\u0026#34;Missing officeId\u0026#34;) return {\u0026#39;statusCode\u0026#39;: 400, \u0026#39;body\u0026#39;: \u0026#39;Missing officeId\u0026#39;} # 3. Create Topic and Payload topic = f\u0026#34;office/{office_id}/room/{room_id}/config\u0026#34; payload = { \u0026#34;command\u0026#34;: \u0026#34;SET_STATE\u0026#34;, \u0026#34;value\u0026#34;: command, \u0026#34;triggeredBy\u0026#34;: \u0026#34;Schedule\u0026#34; } # 4. Send command to IoT Core try: # This line must be aligned with lines above response = iot_client.publish( topic=topic, qos=1, payload=json.dumps(payload) ) logger.info(f\u0026#34;SUCCESS: Sent {command} to {topic}\u0026#34;) return {\u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: \u0026#39;Command sent\u0026#39;} except Exception as e: logger.error(f\u0026#34;IoT Publish Error: {e}\u0026#34;) # Raise error for EventBridge to know it failed (trigger Retry/DLQ) raise e Go to Configuration \u0026ndash;\u0026gt; Permissions and add resource-based policy. (To allow EventBridge to access this lambda function) Add the 2 rules created by AutomationSetup as a trigger for this Lambda. Replace REGION, ACCOUNT, ARNs and resource names with values from your account before running.\n"},{"uri":"https://Son2110.github.io/fcj-workshop/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Smart Office Management System Workshop Overview Smart Office Management System provides a real-time environmental monitoring and management solution for offices, built entirely on AWS Serverless architecture to optimize costs and scalability.\nIn this lab, you will learn how to deploy, configure, and test a full-stack IoT system, allowing sensor devices to transmit data to the cloud and enabling administrators to control devices via a Web Dashboard.\nYou will work with two main architectural models to operate the Smart Office system:\nServerless Architecture - Uses AWS Lambda, API Gateway, and DynamoDB to handle logic and data storage. This model allows code to run in response to requests without managing servers. Event-Driven Architecture - Uses AWS IoT Core, EventBridge, and SNS. The system operates based on events, where data from sensors or user actions trigger automation workflows and send alert notifications. Content Workshop overview Prerequisite Run CloudFormation Stack Set up website Event Bridge SNS Test website IoT connection "},{"uri":"https://Son2110.github.io/fcj-workshop/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at AMAZON WEB SERVICES VIETNAM COMPANY LIMITED from 06/08/2025 to 12/24/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in Smart Office Project, through which I improved my skills in utilizing AWS Services, teamwork, CI/CD pipeline, Web Develope.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ☐ ✅ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively Improve teamwork to finish workshop earlier "},{"uri":"https://Son2110.github.io/fcj-workshop/5-workshop/5.6-sns/","title":"SNS Setup","tags":[],"description":"","content":"Overview This section will guide you through setting up Amazon SNS (Simple Notification Service) to receive alerts from EventBridge when data anomalies are detected.\nCreate SNS Topic Go to Amazon SNS Console Select Topic and name your Topic DON\u0026rsquo;T TURN ON ENCRYPTION\nCreate SNS Subscription After creating the topic, create one or more subscriptions so alerts are delivered to recipients or endpoints (in this case, email).\nSteps:\nSelect Subscription in SNS Choose email protocol, select the topic you just created, and write down the email you want to test. Email endpoints require confirmation before they receive messages. Check your inbox for a confirmation email from AWS SNS and confirm the subscription.\nCreate a rule and attach it to the SNS topic Go to Amazon EventBridge Console Select Rules and create a new rule In step 2, we define the event pattern by choosing Custom pattern. Event patterns can match on source, detail-type.\nUse the following Json:\n{ \u0026#34;source\u0026#34;: [\u0026#34;com.smartoffice.iot\u0026#34;], \u0026#34;detail-type\u0026#34;: [\u0026#34;sensor.anomaly\u0026#34;] } In step 3, Select the target for the rule — choose the SNS topic you created earlier. After creation, EventBridge will forward matching events to the SNS topic which will deliver to its subscriptions.\nReplace email addresses and resource names with values from your account before running.\n"},{"uri":"https://Son2110.github.io/fcj-workshop/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment The office environment is super nice and professional. Everyone at FCJ is friendly and approachable. I never felt scared to ask questions. Plus, working at Bitexco Tower gives a really cool vibe that makes me want to go to work every day.\n2. Support from Mentor / Team Admin My mentor is awesome. Instead of just giving me the answers, they guide me on how to search and solve problems myself, which helps me learn faster. The Admin team is also very helpful with setting up accounts, so I didn\u0026rsquo;t have any issues getting started.\n3. Relevance of Work to Academic Major It fits perfectly with my IT major. At university, I learned a lot of theory, but here I got to practice on real AWS services like EC2, S3, and Bedrock. It bridged the gap between school books and the real world.\n4. Learning \u0026amp; Skill Development Opportunities I didn\u0026rsquo;t just learn about Cloud and Coding. I also learned how to write professional emails, how to write technical blogs, and how to work in a team. The workshops I attended gave me a lot of new keywords to research.\n5. Company Culture \u0026amp; Team Spirit The culture here is \u0026ldquo;Work hard, Play hard.\u0026rdquo; People are serious when working but very fun during breaks. I feel respected even though I\u0026rsquo;m just an intern. There is no toxic hierarchy here.\n6. Internship Policies / Benefits The allowance is fair for an intern. What I appreciate most is the register system, which allows us to balance the register with the office. Also, getting the chance to join tech events is a huge plus!\nAdditional Questions What did you find most satisfying during your internship? The most satisfying thing was successfully deploying my first project on AWS and seeing it run live. Also, being able to attend the \u0026ldquo;AWS Cloud Mastery\u0026rdquo; workshops and meet industry experts was a highlight.\nWhat do you think the company should improve for future interns? None\nIf recommending to a friend, would you suggest they intern here? Why or why not? Definitely, YES. If my friends want to learn about Cloud and DevOps seriously, this is the place. You get to touch real systems, not just fetch coffee.\nSuggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? None.\nWould you like to continue this program in the future? I haven\u0026rsquo;t made a final decision yet. However, I will definitely continue to pursue and upgrade my AWS knowledge.\nAny other comments (free sharing): I just want to say a big thank you to my mentor and the whole team for being so patient with me. This internship has been the best learning experience I\u0026rsquo;ve had so far.\n"},{"uri":"https://Son2110.github.io/fcj-workshop/5-workshop/5.7-test-website-iot-connection/","title":"Test website and IoT connection","tags":[],"description":"","content":"Download mock IoT device script main.py\nAdd data from your mock device ENDPOINT = \u0026#34;\u0026#34; CLIENT_ID = \u0026#34;\u0026#34; OFFICE_ID = \u0026#34;\u0026#34; ROOM_ID = \u0026#34;\u0026#34; PATH_TO_CERT = \u0026#34;\u0026#34; PATH_TO_KEY = \u0026#34;\u0026#34; PATH_TO_ROOT = \u0026#34;\u0026#34; Attribute Value ENDPOINT Website manager page CLIENT_ID ID of your office OFFICE_ID Name of your office ROOM_ID ID of your room PATH_TO_CERT Path to your device certificate download when your create room PATH_TO_KEY Path to your device private key download when your create room PATH_TO_ROOT Path to your Amazon root certifica\tdownload when your create room Link video demo https://www.youtube.com/watch?v=k45jHjkKhuc\n"},{"uri":"https://Son2110.github.io/fcj-workshop/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://Son2110.github.io/fcj-workshop/tags/","title":"Tags","tags":[],"description":"","content":""}]